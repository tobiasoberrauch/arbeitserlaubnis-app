# Ollama Integration - Arbeitserlaubnis App

## ğŸš€ Features

- **Mehrsprachiger Chat**: UnterstÃ¼tzt Deutsch, Englisch, TÃ¼rkisch, Arabisch, Polnisch und Ukrainisch
- **Lokale KI**: Nutzt Ollama mit dem qwen2.5:7b Modell (empfohlen)
- **Streaming Responses**: Echtzeitantworten mit Stream-Support
- **Modellauswahl**: WÃ¤hle zwischen verschiedenen lokalen Modellen
- **Temperature Control**: Anpassbare KreativitÃ¤t der Antworten

## ğŸ“‹ Voraussetzungen

1. **Ollama installiert und lÃ¤uft**
   ```bash
   # PrÃ¼fen ob Ollama lÃ¤uft
   curl http://localhost:11434/api/tags
   ```

2. **Empfohlenes Modell installiert**
   ```bash
   ollama pull qwen2.5:7b
   ```

## ğŸ¯ Verwendung

1. **Entwicklungsserver starten**
   ```bash
   npm run dev
   ```

2. **App Ã¶ffnen**
   - Gehe zu http://localhost:3001
   - Klicke auf "Ollama Chat" Button (unten rechts)

3. **Chat-Interface**
   - WÃ¤hle deine Sprache
   - WÃ¤hle ein Modell aus deiner Ollama-Installation
   - Stelle Fragen zur Arbeitserlaubnis in deiner Sprache

## ğŸ› ï¸ Konfiguration

### Umgebungsvariablen (.env.local)
```env
OLLAMA_HOST=http://localhost:11434  # Standard Ollama Host
```

### VerfÃ¼gbare Modelle

Die App zeigt automatisch alle installierten Ollama-Modelle an. Empfohlen:

- **qwen2.5:7b** - Beste Mehrsprachigkeit (Standard)
- **mistral:latest** - Gute Alternative
- **llama3:8b** - Starke Performance

### API Endpoints

- `GET /api/ollama` - Status und Modellliste
- `POST /api/ollama` - Chat-Anfragen (mit/ohne Streaming)

## ğŸŒ UnterstÃ¼tzte Sprachen

- ğŸ‡©ğŸ‡ª Deutsch
- ğŸ‡¬ğŸ‡§ Englisch  
- ğŸ‡¹ğŸ‡· TÃ¼rkisch
- ğŸ‡¸ğŸ‡¦ Arabisch
- ğŸ‡µğŸ‡± Polnisch
- ğŸ‡ºğŸ‡¦ Ukrainisch

## ğŸ“ Projektstruktur

```
arbeitserlaubnis-app/
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ ollamaService.ts        # Ollama Service Layer
â”œâ”€â”€ app/
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ ollama/
â”‚           â””â”€â”€ route.ts         # API Endpoint
â”œâ”€â”€ components/
â”‚   â””â”€â”€ OllamaChatInterface.tsx # Chat UI Komponente
â””â”€â”€ app/
    â””â”€â”€ page.tsx                 # Integration in Hauptseite
```

## ğŸ”§ Technische Details

### Service Layer (`lib/ollamaService.ts`)
- Verbindung zu Ollama API
- Streaming und normale Chat-Funktionen
- Sprachspezifische System-Prompts
- Modell-Management

### API Route (`app/api/ollama/route.ts`)
- Next.js Route Handler
- Server-Sent Events fÃ¼r Streaming
- Error Handling

### Frontend (`components/OllamaChatInterface.tsx`)
- React Chat-Interface
- Real-time Streaming Display
- Modell- und Temperature-Einstellungen
- Mehrsprachige UI

## ğŸ› Fehlerbehebung

**Ollama nicht erreichbar:**
```bash
# Ollama neustarten
ollama serve

# Status prÃ¼fen
curl http://localhost:11434/api/tags
```

**Modell nicht gefunden:**
```bash
# VerfÃ¼gbare Modelle anzeigen
ollama list

# Neues Modell installieren
ollama pull qwen2.5:7b
```

**Port-Konflikt:**
- Die App nutzt Port 3001 (falls 3000 belegt)
- Ollama Standard-Port: 11434

## ğŸ“ˆ Performance-Tipps

1. **ModellgrÃ¶ÃŸe**: Kleinere Modelle (7B) fÃ¼r schnellere Antworten
2. **Temperature**: Niedrigere Werte (0.3-0.5) fÃ¼r konsistentere Antworten
3. **Streaming**: Aktiviert fÃ¼r bessere UX bei lÃ¤ngeren Antworten

## ğŸ” Sicherheit

- Alle Anfragen laufen lokal
- Keine Daten verlassen deinen Computer
- Ollama lÃ¤uft im lokalen Netzwerk

## ğŸš€ Weitere Entwicklung

MÃ¶gliche Erweiterungen:
- [ ] Chat-Historie speichern
- [ ] Export von GesprÃ¤chen
- [ ] Fine-tuning fÃ¼r Arbeitserlaubnis-Fragen
- [ ] Voice Input/Output
- [ ] Dokumenten-Upload fÃ¼r Kontext